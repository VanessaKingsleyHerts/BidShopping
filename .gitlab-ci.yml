#.gitlab-ci.yml - hybrid

stages:
  - build
  - lint
  - test
  - post_logs

variables:
  PYTHONDONTWRITEBYTECODE: "1"
  PYTHONUNBUFFERED:       "1"
  POSTGRES_DB:            "bidshopping"
  POSTGRES_USER:          "postgres"
  POSTGRES_PASSWORD:      "postgres"
  DATABASE_URL:           "postgres://postgres:postgres@db:5432/bidshopping"
  STATIC_ROOT:            "/tmp/static"
  DOCKER_TLS_CERTDIR:     ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
build-image:
  stage: build
  image: python:3.10-slim    # has Python3 builtâ€‘in
  services:
    - docker:dind
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ""
  before_script:
    # Install Docker CLI + psutil
    - apt-get update && apt-get install -y docker.io
    - python3 -m pip install --no-cache-dir psutil
  script:
    # Run ci_loggerâ€™d Docker commands:
    - python ci/ci_logger.py "docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" \"$CI_REGISTRY\"" --tag build --label docker-login
    - python ci/ci_logger.py "docker build -t \"$CI_REGISTRY_IMAGE/bidshopping:dev\" ." --tag build --label docker-build
    - python ci/ci_logger.py "docker push \"$CI_REGISTRY_IMAGE/bidshopping:dev\"" --tag build --label docker-push
  artifacts:
    when: always
    paths:
      - logs/ci_logs.csv
  only:
    - main

image: "$CI_REGISTRY_IMAGE/bidshopping:dev"
services:
  - name: postgres:13
    alias: db

cache:
  paths:
    - .cache/pip

before_script:
  - mkdir -p "$STATIC_ROOT"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

lint:
  stage: lint
  script:
    - python ci/ci_logger.py "flake8 ." --tag lint
  artifacts:
    when: always
    paths:
      - logs/ci_logs.csv

test:
  stage: test
  script:
    # 1) collectstatic & migrate
    - python ci/ci_logger.py "python manage.py collectstatic --noinput" --tag test
    - python ci/ci_logger.py "python manage.py migrate --noinput" --tag test

    # 2) unit tests
    - python ci/ci_logger.py "coverage run --source=. manage.py test --parallel --exclude-tag=functional" --tag test

    # 3) functional tests
    - python manage.py runserver 0.0.0.0:8000 &
    - sleep 5
    - python ci/ci_logger.py "coverage run --append --source=. manage.py test --tag=functional" --tag test

    # 4) coverage report
    - coverage report
  artifacts:
    when: manual
    paths:
      - htmlcov/
      - functional_tests/screenshots/
      - logs/ci_logs.csv
    expire_in: 1 week

# ðŸ“¥ Download and merge incrementally
update-logs:
  stage: post_logs
  image: python:3.10
  needs:
    - job: build-image
      artifacts: true
      optional: true
    - job: lint
      artifacts: true
      optional: true
    - job: test
      artifacts: true
      optional: true
  when: always
  allow_failure: false
  before_script:
    - pip install pandas requests
  script:
    # 1) Copy this pipeline's log into data/raw/
    - mkdir -p data/raw
    - cp logs/ci_logs.csv data/raw/${CI_PIPELINE_ID}.csv
    # 2) Merge it into data/all_logs.csv
    - python scripts/merge_incremental.py
    # 3) Push back to GitHub via your existing push_to_github.py
    - echo ">>>> Pushing updated all_logs.csv to GitHub"
    - python scripts/push_to_github.py
  only:
    - main
  except:
    - schedules

# ðŸ§¼ Full refresh from all raw logs â€” used for scheduled jobs
collect-logs:
  stage: post_logs
  image: python:3.10
  before_script:
    - pip install requests pandas
  script:
    - python scripts/download_logs.py
    - python scripts/merge_full.py
    - echo ">>> Pushing merged CSV via GitHub API"
    - pip install requests
    - python scripts/push_to_github.py
  only:
    - schedules
  when: always
  allow_failure: false
