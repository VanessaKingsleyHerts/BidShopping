variables:
  ALL_IN_ONE: "false"
  FORCE_RUN: "true"
  HEAL_MODE: "baseline" # baseline|ml
  INJECT_FAIL: "0"
  TEST_SELECTION: "off"     # off|suite|budget
  RISK_THRESHOLD: "0.30"    # used only when TEST_SELECTION=suite # 1.0 (forces functional to be not selected)
  MAX_TEST_MINUTES: "15"    # used only when TEST_SELECTION=budget
  FAIL_NEXT: "0"
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  PYTHONDONTWRITEBYTECODE: "1"
  PYTHONUNBUFFERED: "1"
  BASE_IMG: "registry.gitlab.com/uhthesis/bidshopping:ci-base"

#workflow:
  #rules:
    #- if: '$ALL_IN_ONE == "true"'
      #variables:
        #INJECT_FAIL: "false"
      #when: always
    #- when: never

stages:
  - setup
  - build
  - lint
  - test
  - all_in_one
  - log_collect
  - post_logs

show_env:
  stage: setup
  script:
    - 'echo "Triggered via: $CI_PIPELINE_SOURCE"'
    - 'echo "Schedule name: ${CI_SCHEDULE_NAME:-unset}"'
    - 'echo "Experiment type: ${ALL_IN_ONE:-unset}"'
    - 'echo "HEAL_MODE: ${HEAL_MODE:-unset}"'
    - 'echo "INJECT_FAIL: ${INJECT_FAIL:-unset}"'
  rules:
    - when: never


cache:
  key: "$CI_COMMIT_REF_SLUG"
  paths:
    - .cache/pip/

all_in_one:
  image: "$BASE_IMG"
  stage: all_in_one
  services:
    - docker:dind
  variables:
    DOCKER_HOST: "tcp://docker:2375"
    DOCKER_TLS_CERTDIR: ""
  before_script:
    - mkdir -p logs
    - test -f ci/__init__.py || (echo "ci module is not importable!" && exit 1)
  script:
    - 'echo "=== ALL_IN_ONE MODE: $HEAL_MODE (inject=$INJECT_FAIL) ==="'

    # Safer login
    - python ci/predict_and_heal.py "echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\"" --tag build --label docker-login

    # Docker build pipeline simulating 'build' stage
    - python ci/predict_and_heal.py "docker build -f Dockerfile -t \"$CI_REGISTRY_IMAGE:dev\" ." --tag build --label docker-build
    - python ci/predict_and_heal.py "docker push \"$CI_REGISTRY_IMAGE:dev\"" --tag build --label docker-push

    # Lint and tests
    - python ci/predict_and_heal.py "flake8 ." --tag lint
    - python ci/predict_and_heal.py "python manage.py collectstatic --noinput" --tag test
    - python ci/predict_and_heal.py "python manage.py migrate --noinput" --tag test
    - python ci/predict_and_heal.py "coverage run --source=. manage.py test --parallel --exclude-tag=functional" --tag test
  artifacts:
    when: always
    paths:
      - logs/ci_logs.csv
  rules:
    - if: '$ALL_IN_ONE == "true"'
      when: always
    - when: never

build-image:
  image: "$BASE_IMG"
  stage: build
  services:
    - docker:dind
  variables:
    DOCKER_HOST: "tcp://docker:2375"
    DOCKER_TLS_CERTDIR: ""
  before_script:
    - mkdir -p logs
  script:
    - python ci/predict_and_heal.py "echo \"$CI_REGISTRY_PASSWORD\" | docker login -u \"$CI_REGISTRY_USER\" --password-stdin \"$CI_REGISTRY\"" --tag build --label docker-login
    - python ci/predict_and_heal.py "docker build -f Dockerfile -t \"$CI_REGISTRY_IMAGE:dev\" ." --tag build --label docker-build
    - python ci/predict_and_heal.py "docker push \"$CI_REGISTRY_IMAGE:dev\"" --tag build --label docker-push
    # save a per-job copy to avoid artifact overwrite
    - mkdir -p "logs/$CI_JOB_NAME"
    - test -f logs/ci_logs.csv && cp logs/ci_logs.csv "logs/$CI_JOB_NAME/ci_logs.csv" || true
  artifacts:
    when: always
    paths:
      - logs/$CI_JOB_NAME/ci_logs.csv
  rules:
    - if: '$ALL_IN_ONE == "true"'
      when: never
    - if: '$FORCE_RUN  == "true"'
      when: always
    - changes:
        - Dockerfile
        - requirements*.txt
        - ci/**
      when: on_success
    - exists:
        - ci/
      when: manual

lint:
  image: "$BASE_IMG"
  stage: lint
  script:
    - python ci/predict_and_heal.py "flake8 ." --tag lint
    - mkdir -p "logs/$CI_JOB_NAME"
    - test -f logs/ci_logs.csv && cp logs/ci_logs.csv "logs/$CI_JOB_NAME/ci_logs.csv" || true
  artifacts:
    when: always
    paths:
      - logs/$CI_JOB_NAME/ci_logs.csv
  rules:
    - if: '$ALL_IN_ONE == "true"'
      when: never
    - if: '$FORCE_RUN  == "true"'
      when: always
    - changes:
        - "**/*.py"
        - .flake8
      when: on_success
    - exists:
        - .flake8
      when: manual

unit_tests:
  image: "$BASE_IMG"
  stage: test
  script:
    - python ci/predict_and_heal.py "python manage.py collectstatic --noinput" --tag unit --label collectstatic
    - python ci/predict_and_heal.py "python manage.py migrate --noinput" --tag unit --label migrate
    - python ci/predict_and_heal.py "coverage run --source=. manage.py test --parallel --exclude-tag=functional" --tag unit --label unit-tests
    - mkdir -p "logs/$CI_JOB_NAME"
    - test -f logs/ci_logs.csv && cp logs/ci_logs.csv "logs/$CI_JOB_NAME/ci_logs.csv" || true
    - 'echo "DEBUG list: logs/$CI_JOB_NAME"; ls -l "logs/$CI_JOB_NAME" || true'
    - 'echo "DEBUG head:"; head -n 5 "logs/$CI_JOB_NAME/ci_logs.csv" || true'
  artifacts:
    when: always
    paths:
      - htmlcov/
      - logs/$CI_JOB_NAME/ci_logs.csv
  rules:
    - if: '$ALL_IN_ONE == "true"'
      when: never
    - if: '$FORCE_RUN  == "true"'
      when: always
    - changes:
        - "**/*.py"
        - manage.py
        - requirements*.txt
        - ci/**
      when: on_success
    - exists:
        - auction/tests/
      when: manual

functional_tests:
  image: "$BASE_IMG"
  stage: test
  script:
    - mkdir -p functional_tests/screenshots   # prevents "no matching files" warning when skipped
    # combine server spin-up + tests in a single wrapped command so selection can skip early
    - python ci/predict_and_heal.py "bash -lc 'python manage.py runserver 0.0.0.0:8000 & sleep 5; coverage run --append --source=. manage.py test --tag=functional; coverage report'" --tag functional --label functional-tests
    - mkdir -p "logs/$CI_JOB_NAME"
    - test -f logs/ci_logs.csv && cp logs/ci_logs.csv "logs/$CI_JOB_NAME/ci_logs.csv" || true
    - 'echo "DEBUG list: logs/$CI_JOB_NAME"; ls -l "logs/$CI_JOB_NAME" || true'
    - 'echo "DEBUG head:"; head -n 5 "logs/$CI_JOB_NAME/ci_logs.csv" || true'
  artifacts:
    when: always
    paths:
      - functional_tests/screenshots/
      - logs/$CI_JOB_NAME/ci_logs.csv
  rules:
    - if: '$ALL_IN_ONE == "true"'
      when: never
    - if: '$FORCE_RUN  == "true"'
      when: always
    - changes:
        - auction/templates/**/*
        - auction/static/**/*
        - functional_tests/**/*
      when: on_success
    - exists:
        - functional_tests/
      when: manual

update-logs:
  image: "$BASE_IMG"
  stage: post_logs
  allow_failure: true
  needs:
    - job: build-image
      artifacts: true
      optional: true
    - job: lint
      artifacts: true
      optional: true
    - job: unit_tests
      artifacts: true
      optional: true
    - job: functional_tests
      artifacts: true
      optional: true
    - job: all_in_one
      artifacts: true
      optional: true
  script:
    - mkdir -p data/raw
    # DEBUG: list candidate CSVs (safe—no shell braces)
    - |
      python - <<'PY'
      import glob, os, csv
      from datetime import datetime

      paths = sorted(set(
          glob.glob('logs/*/ci_logs.csv')
          + (['logs/ci_logs.csv'] if os.path.exists('logs/ci_logs.csv') else [])
      ))
      print("DEBUG candidates:", paths or "(none)")
      for p in paths:
          try:
              with open(p) as f:
                  n = sum(1 for _ in f)
              print(f"{p}\t{n} lines")
          except Exception as e:
              print(f"{p}\tERR {e}")

      # Merge per-job logs AND the root logs into one file
      rows, header = [], None
      for p in paths:
          with open(p, newline='') as f:
              r = csv.reader(f)
              h = next(r, None)
              if h and header is None:
                  header = h
              for row in r:
                  rows.append(row)

      if header is None:
          os.makedirs("logs", exist_ok=True)
          with open("logs/ci_logs.csv","w") as f:
              f.write("timestamp,command,duration_s,exit_code,cpu_pct_avg,mem_kb_max,tag,status,pipeline_id,mode\n")
      else:
          def parse_ts(s):
              for fmt in ("%Y-%m-%d %H:%M:%S",):
                  try: return datetime.strptime(s, fmt)
                  except: pass
              try: return datetime.fromisoformat(s)
              except: return datetime.max
          tag_order = {"build":0, "lint":1, "unit":2, "functional":3, "test":2}
          rows.sort(key=lambda r: (parse_ts(r[0]), tag_order.get(r[6], 9), r[1]))
          os.makedirs("logs", exist_ok=True)
          with open("logs/ci_logs.csv","w",newline="") as f:
              w = csv.writer(f)
              w.writerow(header)
              w.writerows(rows)
      PY

    - |
      if [ ! -f logs/ci_logs.csv ]; then
        echo "⚠️ No logs/ci_logs.csv found — skipping logs update"
        exit 0
      fi

    # Copy merged CSV into cohort subfolders
    - |
      case "$(echo "$TEST_SELECTION" | tr '[:upper:]' '[:lower:]')" in
        off) DEST="data/raw/posthealing" ;;
        *)   DEST="data/raw/selection" ;;
      esac
      mkdir -p "$DEST"
      cp logs/ci_logs.csv "$DEST/$CI_PIPELINE_ID.csv"

    # Keep single-file raw archive too
    - cp logs/ci_logs.csv data/raw/$CI_PIPELINE_ID.csv

    # Merge into all_logs and push
    - python scripts/merge_incremental.py || echo "update logs failed"
    - python scripts/push_to_github.py || echo "push failed"
  rules:
    - when: always

collect-logs:
  image: "$BASE_IMG"
  stage: post_logs
  allow_failure: true
  before_script:
    - mkdir -p logs
  script:
    - python scripts/download_logs.py
    - python scripts/merge_full.py
    - python scripts/push_to_github.py
  rules:
    - if: '$ALL_IN_ONE == "true"'
      when: never
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
      when: always
    - when: never
