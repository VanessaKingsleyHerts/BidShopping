#.gitlab-ci.yml - hybrid

stages:
  - build
  - lint
  - test
  - post_logs

variables:
  HEAL_MODE:                "baseline"  # or "ml"
  PYTHONDONTWRITEBYTECODE:  "1"
  PYTHONUNBUFFERED:         "1"
  POSTGRES_DB:              "bidshopping"
  POSTGRES_USER:            "postgres"
  POSTGRES_PASSWORD:        "postgres"
  DATABASE_URL:             "postgres://postgres:postgres@db:5432/bidshopping"
  STATIC_ROOT:              "/tmp/static"
  DOCKER_TLS_CERTDIR:       ""


# â”€â”€â”€ Global setup for all jobs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
before_script:
  # 1) Make logs folder and static root
  - mkdir -p logs
  - mkdir -p "$STATIC_ROOT"
  # 2) Update pip & install Python deps
  - python3 -m pip install --upgrade pip
  - python3 -m pip install --no-cache-dir psutil joblib pandas tensorflow keras
  # 3) Only in build stage: install Docker CLI
  - if [ "$CI_JOB_STAGE" = "build" ]; then apt-get update && apt-get install -y docker.io; fi

# â”€â”€â”€ Per-job definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
build-image:
  stage: build
  image: python:3.10-slim    # has Python3 builtâ€‘in
  services:
    - docker:dind
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_TLS_CERTDIR: ""
  script:
    # Run predict_and_heal on Docker commands:
    - python ci/predict_and_heal.py "docker login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" \"$CI_REGISTRY\"" --tag build --label docker-login
    - python ci/predict_and_heal.py "docker build -t \"$CI_REGISTRY_IMAGE/bidshopping:dev\" ." --tag build --label docker-build
    - python ci/predict_and_heal.py "docker push \"$CI_REGISTRY_IMAGE/bidshopping:dev\"" --tag build --label docker-push
  artifacts:
    when: always
    paths:
      - logs/ci_logs.csv
  only:
    - main

image: "$CI_REGISTRY_IMAGE/bidshopping:dev"
services:
  - name: postgres:13
    alias: db

cache:
  key: "$CI_PIPELINE_ID"
  paths:
    - .cache/pip
    - logs/ci_logs.cs

lint:
  stage: lint
  script:
    - python ci/predict_and_heal.py "flake8 ." --tag lint
  artifacts:
    when: always
    paths:
      - logs/ci_logs.csv

test:
  stage: test
  script:
    # 1) collectstatic & migrate
    - python ci/predict_and_heal.py "python manage.py collectstatic --noinput" --tag test
    - python ci/predict_and_heal.py "python manage.py migrate --noinput" --tag test

    # 2) unit tests
    - python ci/predict_and_heal.py "coverage run --source=. manage.py test --parallel --exclude-tag=functional" --tag test

    # 3) functional tests
    - python manage.py runserver 0.0.0.0:8000 &
    - sleep 5
    - python ci/predict_and_heal.py "coverage run --append --source=. manage.py test --tag=functional" --tag test

    # 4) coverage report
    - coverage report
  artifacts:
    when: always
    paths:
      - htmlcov/
      - functional_tests/screenshots/
      - logs/ci_logs.csv
    # expire_in: 1 week

# ðŸ“¥ Download and merge incrementally
update-logs:
  stage: post_logs
  image: python:3.10
  # needs:
    # - job: build-image
      # artifacts: true
      # optional: true
    # - job: lint
      # artifacts: true
      # optional: true
    # - job: test
      # artifacts: true
      # optional: true
  when: always
  allow_failure: false
  before_script:
    - pip install pandas requests
  script:
    # 1) Copy this pipeline's log into data/raw/
    - mkdir -p data/raw
    - cp logs/ci_logs.csv data/raw/${CI_PIPELINE_ID}.csv
    # 2) Merge it into data/all_logs.csv
    - python scripts/merge_incremental.py
    # 3) Push back to GitHub via your existing push_to_github.py
    - echo ">>>> Pushing updated all_logs.csv to GitHub"
    - python scripts/push_to_github.py
  only:
    - main
  except:
    - schedules

# ðŸ§¼ Full refresh from all raw logs â€” used for scheduled jobs
collect-logs:
  stage: post_logs
  image: python:3.10
  before_script:
    - pip install requests pandas
  script:
    - python scripts/download_logs.py
    - python scripts/merge_full.py
    - echo ">>> Pushing merged CSV via GitHub API"
    - python scripts/push_to_github.py
  only:
    - schedules
  when: manual
  allow_failure: false
